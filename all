Practical 1
Aim: Data Entry and Presentation Functions

x = 3 + 5i
print(!is.numeric(x))
[1] TRUE

x =3
if (x>0) {
  print("Number is positive")
} else {
  print("Number is not positive")
}
[1] "Number is positive

Q) Checking for whether x is a leap year:
x = 2000
if (x%%100==0)
{
  if (x%%400==0){print("Leap year")
    } else {
    print("Not a leap year")}
} else if (x%%4==0) {
  print("Leap year")
} else {
  print("Not a leap year")}
[1] "Leap year"

x <- 0
if (x > 0) {
  print("x is a positive number")
} else if (x < 0) {
  print("x is a negative number")
} else {      #else should compulsory written in this format
  print("x is zero")
}
[1] "x is zero"

x <- -20
if (x > 0) {
  if (x %% 2 == 0) {
    print("x is a positive even number")
  } else {
    print("x is a positive odd number")
  }
} else {
  if (x %% 2 == 0) {
    print("x is a negative even number")
  } else {
    print("x is a negative odd number")
  }
}
[1] "x is a negative even number"

x <- c(12, 9, 23, 14, 20, 1, 5)
ifelse(x %% 2 == 0, "EVEN", "ODD")
[1] "EVEN" "ODD"  "ODD"  "EVEN" "EVEN" "ODD"  "ODD"

num = c(2, 4, 6, 8, 10)
for (x in num) {
  print(x)
}
[1] 2
[1] 4
[1] 6
[1] 8
[1] 10

num = c(2,3,12,14,5,19,23,64)
count = 0
for (i in num) {
  if (i%%2==0) {
    count= count+1
  }
}

x=1
repeat {
  print(x)
  if (x>4) {
    break
  }
  x = x+1
}
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
power <- function(a, b) {
  print(paste("a raised to the power b is: ", a^b))
}
power(2,3)
[1] "a raised to the power b is:  8"

rd = read.csv("US_honey_dataset.csv")
print(rd)
max(rd$production)
min(rd$production)
cat("Max production", max(rd$production))
sum(rd$production)
sub_data = subset(rd, production<100000)
print(sub_data)


print(airquality)
cat("Dimension:",dim(airquality))
cat("\nRow:",nrow(airquality))
cat("\nColumn:",ncol(airquality))
cat("\nName of Variables:",names(airquality))
print(airquality$Temp)
sort(airquality$Temp)


# Create a data frame
dataframe1 <- data.frame (
  Name = c("Juan", "Kay", "Jay", "Ray", "Aley"),
  Age = c(22, 15, 19, 30, 23),
  ID = c(101, 102, 103, 104, 105)
)
print(dataframe1)
print(max(dataframe1$Age)) # 30
print(min(dataframe1$ID)) # 101


# vector of marks
marks <- c(97, 78, 57,78, 97, 66, 87, 64, 87, 78)
mode = function() {
  return(names(sort(-table(marks)))[1])
}
# call mode() function
mode()


Practical 3:
# Sample dataset
data <- data.frame(
  ID = 1:5,
  Math = c(70, 85, 60, 90, 75),
  Science = c(80, 78, 72, 95, 88)
)
print(data)

# Add new column: Total
data$Total <- data$Math + data$Science

# Average marks
data$Average <- (data$Math + data$Science) / 2

# Difference between Math and Science
data$Diff <- data$Math - data$Science

# Product of Math and Science scores
data$Product <- data$Math * data$Science

# Square, square root, log
data$Math_Square <- data$Math^2
data$Science_Sqrt <- sqrt(data$Science)
data$Math_Log <- log(data$Math)  # natural log

# Percentage (out of 200)
data$Percentage <- (data$Total / 200) * 100

# Basic statistics
mean_math <- mean(data$Math)
sd_science <- sd(data$Science)
var_math <- var(data$Math)

cat("Mean (Math):", mean_math, "\n")
cat("SD (Science):", sd_science, "\n")
cat("Variance (Math):", var_math, "\n")

# Final dataset
print(data)


Practical 5:Correlation 

Correlation:
x <-  c(2,3,4,5,7)
y <- c(3,4,4,6,8)
result <- cor(x,y,method="kendall")
result
Output: [1] 0.9486833

x <-  c(2,3,4,5,7)
y <- c(3,4,4,6,8) 
z = cor.test(x,y,method="pearson")
p = cov(x,y,method = 'pearson')
z 
p [1] 3.75
Output:	Pearson's product-moment correlation

data:  x and y
t = 7.5633, df = 3, p-value = 0.004794
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.6607278 0.9984028
sample estimates:
      cor 
0.9747661
 #Pearson - ‘When data is linear, continuous’ Kendal - ‘When data is irrelevant to each other’
#ex 2.3.1
x <- c(65,66,67,67,68,69,70,72)
y <- c(67,68,65,68,72,72,69,71)
z = cor(x,y,method='pearson')
cat("correlation between x and y is :",z)
Output: correlation between x and y is : 0.6030227

#ex 2.3.3
x<- c(21,22,23,24,25,26,27)
y<- c(16,15,17,18,19,20,21)
z = cor(x,y, method='pearson')
cat("correlation between x and y is :",z)
Output:correlation between x and y is : 0.9642857

#ex2.3.4
mid_val <- c(10,30,50,70,90)
y <- c(350,280,540,760,900)
z = cor(mid_val,y,method="pearson")
cat("correlation between x and y is :",z)
Output: correlation between x and y is : 0.9470761

#ex 2.2
#q1
x<- c(1,3,4,5,7,8,10)
y <- c(2,6,8,10,14,16,20)
z<- cor(x,y,method = 'pearson')
cat("correlation between x and y is :", z)
Output: correlation between x and y is : 1

#q2 
x<- c(-10,-5,0,5,10)
y<- c(5,9,7,11,13)
z <- cor(x,y,method = 'pearson')
cat("correlation between x and y is :",z)
Output: correlation between x and y is : 0.9

#q3
x<-c(75,30,60,80,53,35,15,40,38,48)
y <- c(85,45,54,91,58,63,35,43,45,44)
z <- cor(x,y,method = 'pearson')
cat("correlation between x and y is :",z)
Output: correlation between x and y is : 0.8593494

   
   x <- c(56,425,72,36,63,47,55,49,38,42,68,60)
   y <- c(147,125,160,118,149,128,150,145,115,140,152,155)
   x_bar <- mean(x)
   y_bar <- mean(y)
   b_yx <- sum((x - x_bar) * (y - y_bar)) / sum((x - x_bar)^2)
   b_yx
[1] -0.03206863
 
   a_yx <- y_bar - b_yx * x_bar
  a_yx
[1] 143.0351
     cat("Regression line of Y on X: Y =", round(a_yx, 2), "+", round(b_yx, 2), "* X\n")
Regression line of Y on X: Y = 143.04 + -0.03 * X
 
   b_xy <- sum((x - x_bar) * (y - y_bar)) / sum((y - y_bar)^2)
   b_xy
[1] -1.642762
   
   a_xy <- x_bar - b_xy * y_bar
   
   cat("Regression line of X on Y: X =", round(a_xy, 2), "+", round(b_xy, 2), "* Y\n")
Regression line of X on Y: X = 314.78 + -1.64 * Y
   
   # Y on X
   model_yx <- lm(y ~ x)
   cat("Y on X: Y =", coef(model_yx)[1], "+", coef(model_yx)[2], "* X\n")
Y on X: Y = 143.0351 + -0.03206863 * X
   
   # X on Y
   model_xy <- lm(x ~ y)
   cat("X on Y: X =", coef(model_xy)[1], "+", coef(model_xy)[2], "* Y\n")
X on Y: X = 314.7843 + -1.642762 * Y

Practical 6
Aim: Probability and Conditional Probability Functions
Exercise 1: Two Dice Rolled
Find P(sum = 10), P(sum ≥ 10), and P(sum = 10 | sum ≥ 10):
outcomes <- expand.grid(1:6, 1:6)
 sums <- rowSums(outcomes)

 p_exact_10 <- mean(sums == 10)
 p_atleast_10 <- mean(sums >= 10)
 p_conditional <- mean(sums == 10 & sums >= 10) / p_atleast_10

 cat("P(sum = 10):", p_exact_10, "\n")
P(sum = 10): 0.08333333 
 Cat("P(sum ≥ 10):", p_atleast_10, "\n")
P(sum = 10): 0.1666667 
cat("P(sum = 10 | sum ≥ 10):", p_conditional, "\n")
P(sum = 10 | sum = 10): 0.5 

Exercise 2: Drawing Slips from a Hat
A hat contains slips numbered 1 to 6. Two are drawn without replacement. Find same probabilities as in Exercise 1:
outcomes <- combn(1:6, 2)
 sums <- colSums(outcomes)

 p_sum_10 <- mean(sums == 10)
 p_sum_atleast_10 <- mean(sums >= 10)
 p_sum_10_given_10plus <- p_sum_10 / p_sum_atleast_10

 cat("P(sum = 10):", p_sum_10, "\n")
 cat("P(sum ≥ 10):", p_sum_atleast_10, "\n")
 cat("P(sum = 10 | sum ≥ 10):", p_sum_10_given_10plus, "\n")
cat("P(sum = 10):", p_sum_10, "\n")
P(sum = 10): 0.06666667 
 cat("P(sum ≥ 10):", p_sum_atleast_10, "\n")
P(sum = 10): 0.1333333 
 cat("P(sum = 10 | sum ≥ 10):", p_sum_10_given_10plus, "\n")
P(sum = 10 | sum = 10): 0.5 

Exercise 3: One Die is Twice the Other
Find the probability that one die shows a number that is twice the other:
outcomes <- expand.grid(1:6, 1:6)
 p <- mean(outcomes$Var1 == 2 * outcomes$Var2 | outcomes$Var2 == 2 * outcomes$Var1)
cat("P(one die is twice the other):", p, "\n")
P(one die is twice the other): 0.1666667 

Exercise 4: Difference of Dice
Find the probability that the difference is 0 and the probability that it is 4:
diffs <- abs(outcomes$Var1 - outcomes$Var2)
 p_0 <- mean(diffs == 0)
 p_4 <- mean(diffs == 4)
 cat("P(diff = 0):", p_0, "\n")
 cat("P(diff = 4):", p_4, "\n")
P(diff = 0): 0.1666667 
P(diff = 4): 0.1111111 

Exercise 5: Events A, B, C, D
Event A: sum = 7, B: white die odd, C: red > white, D: both equal. Check disjoint and independence:
outcomes <- expand.grid(white = 1:6, red = 1:6)
 A <- outcomes$white + outcomes$red == 7
 B <- outcomes$white %% 2 == 1
 C <- outcomes$red > outcomes$white
 D <- outcomes$white == outcomes$red

 intersect_AB <- A & B
 cat("Are A and B disjoint?", all(!intersect_AB), "\n")

 indep <- mean(A & B) == mean(A) * mean(B)
 cat("Are A and B independent?", indep, "\n")
Are A and B disjoint? FALSE 
Are A and B independent? TRUE 



Practical 7
Aim: Binomial and Poisson Distribution and plotting of its pdf, cdf and pmf Functions

Code:
#EX1
pbinom(5,100,0.05, lower.tail = FALSE)
pbinom(10,100,0.05, lower.tail = FALSE)

pbinom(15,100,0.05, lower.tail = FALSE)
pbinom(5,100,0.05, lower.tail = FALSE)
[1] 0.3840009
pbinom(10,100,0.05, lower.tail = FALSE)
[1] 0.01147241
pbinom(15,100,0.05, lower.tail = FALSE)
[1] 3.705408e-05
#EX2
#X>1
pbinom(1,50,0.02, lower.tail = FALSE)
#x>=3
pbinom(2,50,0.02, lower.tail = FALSE)
#X>1
pbinom(1,50,0.02, lower.tail = FALSE)
[1] 0.2642286
#x>=3
pbinom(2,50,0.02, lower.tail = FALSE)
[1] 0.07842775
#EX 3
pbinom(1,20,0.04, lower.tail = FALSE)
pbinom(1,20,0.04, lower.tail = FALSE)
[1] 0.1896622

#EX 5
#x>5
pbinom(5,50,0.1, lower.tail = FALSE)  
#X<20
pbinom(19,50,0.1, lower.tail =FALSE) 	
#x>5
pbinom(5,50,0.1, lower.tail = FALSE)  
[1] 0.383877
#X<20
pbinom(19,50,0.1, lower.tail =FALSE) 	
[1] 2.368599e-08

EX1
dpois(0,10)
[1] 4.539993e-05

EX2
ppois(110,100,lower.tail = FALSE)
[1] 0.1471373
 ppois(2,1, lower.tail = FALSE)
[1] 0.0803014

#dpois() → Probability Mass Function
x <- 0:10
pmf <- dpois(x, lambda = 5)

plot(x, pmf,
     type = "h",   # histogram-like vertical lines
     lwd = 3,      # line width
     main = "Poisson PMF (λ=5)",
     xlab = "Number of events (x)",
     ylab = "P(X = x)")

# Plot
plot(x, y, type = "h", lwd = 2, col = "blue",
     main = paste("Poisson Distribution (?? =", lambda, ")"),
     xlab = "Number of Events (x)", ylab = "Probability",
     ylim = c(0, max(y) + 0.02))
points(x, y, pch = 16, col = "red")
Output:



Practical 8
Aim: Normal Distribution and plotting of its pdf, cdf, pmf function

Code:
# real life example of normal distribution
#EX1
rnorm(10, 1)
Output:
[1]  0.44415887  2.78691314  1.49785048 -0.96661716  1.70135590
 [6]  0.52720859 -0.06782371  0.78202509 -0.02600445  0.27110877

#standard normal distribution problem and solution pdf
#p1
pnorm(70,50,15)- pnorm(49,50,15)
[1] 0.4353652

#P2
pnorm(100,90,10,lower.tail = FALSE)
[1] 0.1586553

#EX 1
pnorm(39,30,4)
[1] 0.9877755

pnorm(21,30,4 , lower.tail = FALSE)
[1] 0.9877755

pnorm(34,30,4) - pnorm(30,30,4, lower.tail = FALSE)
[1] 0.3413447

#EX1
pnorm(100, 90,10, lower.tail = FALSE)
[1] 0.1586553

#EX2
pnorm(69, 50,15)-pnorm(50, 50, 15, lower.tail = FALSE)
[1] 0.3973627

#EX3
pnorm(585,500,100)
[1] 0.8023375

#ex 4
pnorm(5.02,5,0.02)-pnorm(4.97,5,0.02)
[1] 0.7745375

#EX 5
pnorm(12,12,2)-pnorm(6,12,2)
[1] 0.4986501

# Normal Distribution problems

# Helper normal distribution probabilities
pnorm_less <- function(x, mean, sd) pnorm(x, mean, sd)
pnorm_greater <- function(x, mean, sd) 1 - pnorm(x, mean, sd)
pnorm_between <- function(x1,?x2, mean, sd) pnorm(x2, mean, sd) - pnorm(x1, mean, sd)

# 1) Normal ??=30, ??=4
mu_n1 <- 30
sd_n1 <- 4
P_x_less_40 <- pnorm_less(40, mu_n1, sd_n1)
P_x_greater_21 <- pnorm_greater(21, mu_n1, sd_n1)
P_between_30_35 <- pnorm_between(30, 35, mu_n1, sd_n1)

# 2) Speeds: ??=90, ??=10, P(X>100)
mu_n2 <- 90
sd_n2 <- 10
P_speed_gt_100 <- pnorm_greater(100, mu_n2, sd_n2)

# 3) Battery time ??=50, ??=15, P(50 < X < 70)
mu_n3 <- 50
sd_n3 <- 15
P_battery_50_70 <- pnorm_between(50, 70, mu_n3, sd_n3)


Practical 9
Aim: T-test

Code:
Aim:

Code:
res <- t.test(mice$weight, mu = 25)
res

my_data <- c(14, 14, 16, 13, 12, 17, 15, 14, 15, 13, 15, 14)
t.test(my_data, mu=15)

Output: #One Sample t-test
data:  sample_data
t = 1.1531, df = 9, p-value = 0.2786
alternative hypothesis: true mean is not equal to 18
95 percent confidence interval:
  15.88408 24.51592
sample estimates:
  mean of x 
20.2 ACCEPT H0

sample_data <- c(12, 14, 15, 17, 19, 20, 22, 25, 28, 30)
hypothesized_mean <- 18
t_test_result <- t.test(sample_data, mu = hypothesized_mean)
print(t_test_result)

dat1 <- data.frame(
  value = c(0.9, -0.8, 1.3, -0.3, 1.7)
)
dat1
TEST<- t.test(dat1,mu=1)

x <- c(3, 7, 11, 0, 7, 0, 4, 5, 6, 2)
t.test(x)
Output:
data:  x
t = 4.1367, df = 9, p-value = 0.002534
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  2.0392 6.9608
sample estimates:
  mean of x 
4.5 REJECT H0

miceweight<-c(12,13,12,14,13,12,12,15,16,12,13,12,11,11,12)
data<- t.test(miceweight,mu=12)
data
Output:
data:  miceweight
t = 1.8479, df = 14, p-value = 0.08586
alternative hypothesis: true mean is not equal to 12
95 percent confidence interval:
  11.89288 13.44045
sample estimates:
  mean of x 
12.66667 FAIL TO REJECT H0

set.seed(150)
data <- data.frame(Value = rnorm(30, mean = 50, sd = 10))
data
test <- t.test(data$Value, mu = 50)
test
Output: 
	data:  data$Value
t = 0.57321, df = 29, p-value = 0.5709
alternative hypothesis: true mean is not equal to 50
95 percent confidence interval:
  47.02585 55.29045
sample estimates:
  mean of x 
51.15815 Accept
 Paired T-test

before = c(157,160,158, 152, 154, 156, 151)
after = c(160,164,161, 155,155,157,156)
t.test(before, after, paired = TRUE)

	Paired t-test

data:  before and after
t = -5.164, df = 6, p-value = 0.002087
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -4.210978 -1.503307
sample estimates:
mean difference 
      -2.857143

set.seed(123)
before = rnorm(7, mean = 50000, sd = 50)  # Generate 7 random numbers with mean 50000 and sd 50
after = rnorm(7, mean = 50075, sd = 50)   # Generate 7 random numbers with mean 50075 and sd 50
t.test(before, after, paired = TRUE)

	Paired t-test

data:  before and after
t = -2.6102, df = 6, p-value = 0.04011
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -97.618586  -3.152003
sample estimates:
mean difference 
      -50.38529


